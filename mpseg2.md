---
title: 最大概率汉语切分算法研究（二）BiGram语言模型
date: 2017-11-25 16:51:06
tags: [NLP]
---
BiGram 语言模型，也就是二元语法模型，起源于 NGram，属于 N = 2的情况。基本思想是当前词依赖于仅前一个词的出现概率。

<!--more-->
# 模型简介
N 元语法模型属于概率模型的一种。在 Ngram 语言模型的概念里，我们假设当前词出现的概率依赖于前面`N-1`个单词。

## 概率推导

假设有单词序列$w_1,w_2,…,w_n$,将每个单词在它本身位置的出现看成一个独立事件，则这个单词串出现的概率可以表示为$P(w_1,w_2,…,w_n)$

一般情况下将这个单词序列简记为$w_1^n$，由概率的链式分解规则，有

$P(w_1^n) = P(w_1)P(w_2|w_1)P(w_3|w_1^2)···P(w_n|w_1^{n-1})$

在概率推导公式中，令$n=2$，式子可以简化为$P(w_1^n)=P(w_n|w_{n-1})$

## 说人话？？？

上面的概率推导很容易让人一头雾水，以“我喜欢你”这个句子为例，“我喜欢你”这个句子出现的概率，可以转化成“我喜欢”这个子句后面出现“你”这个词的概率，在二元语法模型中，就可以进一步等价成“喜欢”这个词后面出现“你”的概率。

也就是说，二元语法模型，将一句话出现的可能性，转化成一系列词语搭配出现的可能性。

# 模型训练

由频率估计概率的方法，对于给定的语料库，不难得出如下公式：

$P(w_n|w_{n-1})=\frac{C(w_{n-1}w_n)}{C(w_n)}$

还是以“喜欢你”这个句子为例，可以得到：

$P(你|喜欢)=\frac{"喜欢你"这个搭配在语料库中的出现次数}{所有以“喜欢”开头的双词搭配总数}$

因此，训练的目标就是对于给定的语料库和词典，统计整个语料库中的所有双词搭配和对于具体的每一个词，统计以该词开头的所有双词搭配数目。也就是一个数数的过程。

# 平滑技术

上面提及的 N 元语法的问题在于“数数”这个过程对语料库是强依赖的，而每个特定的语料库都是有限的，肯定无法覆盖汉语中出现的所有双词搭配。所以便有了平滑技术，为“零概率的二元语法”指派非零概率，也就是在计数后进行概率转换之前为计数为0的指派一个非零的计数值。

在这里使用了最简单的加一平滑，在二元计数矩阵中，归一化计算概率之前为所有的计数加一。

# 计算机编程实现

## 模型表示

在开发分词工具中，采用了 pandas 库提供的 DataFrame 这个二维的数据接口来表示整个模型，`index`部分是词典中的每一个词，`col`部分包含了两列，第一列是每个词出现的总数，第二列以字典类型来表示每个词所有可能的“下一个词”和对应出现次数。

### 平滑技术的处理

假设词典里面有 $m$个词，按照上面的描述，需要一个$m \times m$的矩阵来作为二元语法矩阵，加一平滑的时候遍历整个矩阵为每个元素加一。然而，从199801这个语料库中统计出来的词个数已经高达50000+，加载这么高维度的矩阵对于普通计算机来说显然是一笔很大的消耗。

而在这个模型表示中，只储存了所有出现过的“双词”搭配，将对未出现的搭配“赋值为1”的过程放在了概率计算部分，加快了模型的加载速度。

# 参考资料

[JMBook-自然语言处理综论](https://book.douban.com/subject/1390499/)